"""
æ•°æ®å¤‡ä»½æ¨¡å—

Author: Smart Stock Insider Team
Version: 1.0.0
"""

import os
import shutil
import sqlite3
import logging
import asyncio
import gzip
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, List
from zipfile import ZipFile, ZIP_DEFLATED

from core.config import settings
from core.cache import cache_manager

logger = logging.getLogger(__name__)


class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨"""

    def __init__(self):
        self.backup_dir = Path(settings.BACKUP_PATH)
        self.backup_interval = timedelta(hours=settings.BACKUP_INTERVAL)
        self.max_backups = 30  # æœ€å¤šä¿ç•™30ä¸ªå¤‡ä»½
        self.compression_enabled = True

    async def initialize(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        try:
            # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
            self.backup_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºå­ç›®å½•
            (self.backup_dir / "database").mkdir(exist_ok=True)
            (self.backup_dir / "config").mkdir(exist_ok=True)
            (self.backup_dir / "logs").mkdir(exist_ok=True)

            logger.info(f"âœ… å¤‡ä»½ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œå¤‡ä»½ç›®å½•: {self.backup_dir}")

        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def create_full_backup(self, description: Optional[str] = None) -> Dict[str, Any]:
        """
        åˆ›å»ºå®Œæ•´å¤‡ä»½

        Args:
            description: å¤‡ä»½æè¿°

        Returns:
            å¤‡ä»½ä¿¡æ¯å­—å…¸
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}"
        backup_path = self.backup_dir / backup_name

        try:
            backup_path.mkdir(exist_ok=True)

            # å¤‡ä»½æ•°æ®åº“
            db_result = await self._backup_database(backup_path)

            # å¤‡ä»½é…ç½®æ–‡ä»¶
            config_result = await self._backup_config(backup_path)

            # å¤‡ä»½æ—¥å¿—æ–‡ä»¶
            logs_result = await self._backup_logs(backup_path)

            # å¤‡ä»½ç¼“å­˜æ•°æ®ï¼ˆå¯é€‰ï¼‰
            cache_result = await self._backup_cache(backup_path)

            # åˆ›å»ºå¤‡ä»½å…ƒæ•°æ®
            metadata = {
                "backup_name": backup_name,
                "timestamp": timestamp,
                "description": description or f"è‡ªåŠ¨å¤‡ä»½ - {timestamp}",
                "components": {
                    "database": db_result,
                    "config": config_result,
                    "logs": logs_result,
                    "cache": cache_result
                },
                "total_size": self._calculate_backup_size(backup_path),
                "created_at": datetime.now().isoformat()
            }

            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = backup_path / "metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            # å‹ç¼©å¤‡ä»½
            if self.compression_enabled:
                await self._compress_backup(backup_path)

            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups()

            logger.info(f"âœ… å®Œæ•´å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_name}")
            return metadata

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå®Œæ•´å¤‡ä»½å¤±è´¥: {e}")
            # æ¸…ç†å¤±è´¥çš„å¤‡ä»½
            if backup_path.exists():
                shutil.rmtree(backup_path)
            raise

    async def _backup_database(self, backup_path: Path) -> Dict[str, Any]:
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            db_backup_dir = backup_path / "database"
            db_backup_dir.mkdir(exist_ok=True)

            # å¤‡ä»½SQLiteæ•°æ®åº“
            db_source = Path("data/smart_stock.db")
            if db_source.exists():
                db_target = db_backup_dir / "smart_stock.db"
                shutil.copy2(db_source, db_target)

                # éªŒè¯å¤‡ä»½
                if self._verify_sqlite_backup(db_target):
                    size = db_target.stat().st_size
                    logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {size} bytes")
                    return {
                        "status": "success",
                        "size": size,
                        "path": str(db_target.relative_to(backup_path)),
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    raise Exception("æ•°æ®åº“å¤‡ä»½éªŒè¯å¤±è´¥")
            else:
                raise Exception("æºæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _backup_config(self, backup_path: Path) -> Dict[str, Any]:
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        try:
            config_backup_dir = backup_path / "config"
            config_backup_dir.mkdir(exist_ok=True)

            backed_up_files = []
            total_size = 0

            # éœ€è¦å¤‡ä»½çš„é…ç½®æ–‡ä»¶
            config_files = [
                ".env",
                "config.json",
                "alembic.ini",
                "backend/alembic.ini"
            ]

            for config_file in config_files:
                source_path = Path(config_file)
                if source_path.exists():
                    target_path = config_backup_dir / config_file
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(source_path, target_path)

                    size = target_path.stat().st_size
                    backed_up_files.append({
                        "file": config_file,
                        "size": size,
                        "path": str(target_path.relative_to(backup_path))
                    })
                    total_size += size

            logger.info(f"âœ… é…ç½®æ–‡ä»¶å¤‡ä»½æˆåŠŸ: {len(backed_up_files)} ä¸ªæ–‡ä»¶, {total_size} bytes")
            return {
                "status": "success",
                "files": backed_up_files,
                "total_size": total_size,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶å¤‡ä»½å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _backup_logs(self, backup_path: Path) -> Dict[str, Any]:
        """å¤‡ä»½æ—¥å¿—æ–‡ä»¶"""
        try:
            logs_backup_dir = backup_path / "logs"
            logs_backup_dir.mkdir(exist_ok=True)

            backed_up_files = []
            total_size = 0

            # éœ€è¦å¤‡ä»½çš„æ—¥å¿—ç›®å½•
            log_dirs = [
                "logs",
                "backend/logs"
            ]

            for log_dir in log_dirs:
                source_dir = Path(log_dir)
                if source_dir.exists() and source_dir.is_dir():
                    target_dir = logs_backup_dir / log_dir.replace("/", "_")
                    shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)

                    # è®¡ç®—å¤§å°
                    for root, dirs, files in os.walk(target_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            total_size += os.path.getsize(file_path)

                    backed_up_files.append({
                        "directory": log_dir,
                        "target": str(target_dir.relative_to(backup_path))
                    })

            logger.info(f"âœ… æ—¥å¿—æ–‡ä»¶å¤‡ä»½æˆåŠŸ: {len(backed_up_files)} ä¸ªç›®å½•, {total_size} bytes")
            return {
                "status": "success",
                "directories": backed_up_files,
                "total_size": total_size,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ æ—¥å¿—æ–‡ä»¶å¤‡ä»½å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def _backup_cache(self, backup_path: Path) -> Dict[str, Any]:
        """å¤‡ä»½ç¼“å­˜æ•°æ®"""
        try:
            cache_backup_dir = backup_path / "cache"
            cache_backup_dir.mkdir(exist_ok=True)

            # å¯¼å‡ºRedisæ•°æ®
            if cache_manager.redis_client:
                cache_file = cache_backup_dir / "redis_dump.rdb"
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦ä½¿ç”¨Redisçš„BGSAVEå‘½ä»¤
                # æˆ–è€…ä½¿ç”¨Redisçš„DUMP/RESTOREå‘½ä»¤æ¥å¤‡ä»½ç‰¹å®šé”®

                # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
                stats = await cache_manager.get_stats()
                stats_file = cache_backup_dir / "cache_stats.json"
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(stats, f, ensure_ascii=False, indent=2)

                logger.info("âœ… ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯å¤‡ä»½æˆåŠŸ")
                return {
                    "status": "success",
                    "stats": stats,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                logger.warning("âš ï¸ Redisæœªè¿æ¥ï¼Œè·³è¿‡ç¼“å­˜å¤‡ä»½")
                return {
                    "status": "skipped",
                    "reason": "Redisæœªè¿æ¥",
                    "timestamp": datetime.now().isoformat()
                }

        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜å¤‡ä»½å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def _verify_sqlite_backup(self, backup_path: Path) -> bool:
        """éªŒè¯SQLiteå¤‡ä»½"""
        try:
            with sqlite3.connect(str(backup_path)) as conn:
                cursor = conn.cursor()
                cursor.execute("PRAGMA integrity_check")
                result = cursor.fetchone()
                return result[0] == "ok"
        except Exception as e:
            logger.error(f"SQLiteå¤‡ä»½éªŒè¯å¤±è´¥: {e}")
            return False

    def _calculate_backup_size(self, backup_path: Path) -> int:
        """è®¡ç®—å¤‡ä»½å¤§å°"""
        total_size = 0
        for root, dirs, files in os.walk(backup_path):
            for file in files:
                file_path = os.path.join(root, file)
                total_size += os.path.getsize(file_path)
        return total_size

    async def _compress_backup(self, backup_path: Path):
        """å‹ç¼©å¤‡ä»½"""
        try:
            zip_path = backup_path.with_suffix('.zip')

            with ZipFile(zip_path, 'w', ZIP_DEFLATED) as zipf:
                for root, dirs, files in os.walk(backup_path):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, backup_path)
                        zipf.write(file_path, arcname)

            # åˆ é™¤æœªå‹ç¼©çš„å¤‡ä»½ç›®å½•
            shutil.rmtree(backup_path)

            logger.info(f"âœ… å¤‡ä»½å‹ç¼©æˆåŠŸ: {zip_path.name}")

        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å‹ç¼©å¤±è´¥: {e}")

    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            backups = []
            for item in self.backup_dir.iterdir():
                if item.is_dir() and item.name.startswith('backup_'):
                    backups.append(item)
                elif item.is_file() and item.name.startswith('backup_') and item.suffix == '.zip':
                    backups.append(item)

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            backups.sort(key=lambda x: x.stat().st_mtime, reverse=True)

            # ä¿ç•™æœ€æ–°çš„Nä¸ªå¤‡ä»½
            if len(backups) > self.max_backups:
                for old_backup in backups[self.max_backups:]:
                    if old_backup.is_dir():
                        shutil.rmtree(old_backup)
                    else:
                        old_backup.unlink()
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ—§å¤‡ä»½: {old_backup.name}")

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§å¤‡ä»½å¤±è´¥: {e}")

    async def restore_backup(self, backup_name: str, components: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        æ¢å¤å¤‡ä»½

        Args:
            backup_name: å¤‡ä»½åç§°
            components: è¦æ¢å¤çš„ç»„ä»¶åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨

        Returns:
            æ¢å¤ç»“æœ
        """
        try:
            # æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
            backup_path = self._find_backup(backup_name)
            if not backup_path:
                raise Exception(f"å¤‡ä»½ä¸å­˜åœ¨: {backup_name}")

            # è¯»å–å…ƒæ•°æ®
            metadata = self._load_backup_metadata(backup_path)

            # æ¢å¤ç»„ä»¶
            restore_results = {}
            if components is None:
                components = list(metadata["components"].keys())

            for component in components:
                if component in metadata["components"]:
                    result = await self._restore_component(backup_path, component)
                    restore_results[component] = result
                else:
                    restore_results[component] = {
                        "status": "failed",
                        "error": "ç»„ä»¶ä¸å­˜åœ¨äºå¤‡ä»½ä¸­"
                    }

            logger.info(f"âœ… å¤‡ä»½æ¢å¤å®Œæˆ: {backup_name}")
            return {
                "backup_name": backup_name,
                "restored_components": restore_results,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½æ¢å¤å¤±è´¥: {e}")
            raise

    def _find_backup(self, backup_name: str) -> Optional[Path]:
        """æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶"""
        # é¦–å…ˆæŸ¥æ‰¾å‹ç¼©æ–‡ä»¶
        zip_path = self.backup_dir / f"{backup_name}.zip"
        if zip_path.exists():
            return zip_path

        # ç„¶åæŸ¥æ‰¾ç›®å½•
        dir_path = self.backup_dir / backup_name
        if dir_path.exists() and dir_path.is_dir():
            return dir_path

        return None

    def _load_backup_metadata(self, backup_path: Path) -> Dict[str, Any]:
        """åŠ è½½å¤‡ä»½å…ƒæ•°æ®"""
        if backup_path.suffix == '.zip':
            # ä»ZIPæ–‡ä»¶ä¸­è¯»å–å…ƒæ•°æ®
            with ZipFile(backup_path, 'r') as zipf:
                with zipf.open('metadata.json') as f:
                    return json.load(f)
        else:
            # ä»ç›®å½•ä¸­è¯»å–å…ƒæ•°æ®
            metadata_path = backup_path / "metadata.json"
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)

    async def _restore_component(self, backup_path: Path, component: str) -> Dict[str, Any]:
        """æ¢å¤ç‰¹å®šç»„ä»¶"""
        try:
            if backup_path.suffix == '.zip':
                # ä»ZIPæ–‡ä»¶æ¢å¤
                with ZipFile(backup_path, 'r') as zipf:
                    # è¿™é‡Œéœ€è¦å®ç°ä»ZIPæ¢å¤çš„é€»è¾‘
                    pass
            else:
                # ä»ç›®å½•æ¢å¤
                component_dir = backup_path / component

                if component == "database":
                    return await self._restore_database(component_dir)
                elif component == "config":
                    return await self._restore_config(component_dir)
                elif component == "logs":
                    return await self._restore_logs(component_dir)
                elif component == "cache":
                    return await self._restore_cache(component_dir)

        except Exception as e:
            logger.error(f"âŒ æ¢å¤ç»„ä»¶ {component} å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e)
            }

    async def _restore_database(self, backup_dir: Path) -> Dict[str, Any]:
        """æ¢å¤æ•°æ®åº“"""
        try:
            source_db = backup_dir / "smart_stock.db"
            target_db = Path("data/smart_stock.db")

            if source_db.exists():
                # åˆ›å»ºç›®æ ‡ç›®å½•
                target_db.parent.mkdir(parents=True, exist_ok=True)

                # å¤‡ä»½å½“å‰æ•°æ®åº“
                if target_db.exists():
                    backup_current = target_db.with_suffix('.db.backup')
                    shutil.copy2(target_db, backup_current)

                # æ¢å¤æ•°æ®åº“
                shutil.copy2(source_db, target_db)

                # éªŒè¯æ¢å¤
                if self._verify_sqlite_backup(target_db):
                    logger.info("âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ")
                    return {"status": "success"}
                else:
                    raise Exception("æ•°æ®åº“æ¢å¤éªŒè¯å¤±è´¥")
            else:
                raise Exception("å¤‡ä»½æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æ¢å¤å¤±è´¥: {e}")
            return {
                "status": "failed",
                "error": str(e)
            }

    async def get_backup_list(self) -> List[Dict[str, Any]]:
        """è·å–å¤‡ä»½åˆ—è¡¨"""
        backups = []

        for item in self.backup_dir.iterdir():
            if item.name.startswith('backup_'):
                try:
                    if item.suffix == '.zip':
                        metadata = self._load_backup_metadata(item)
                    elif item.is_dir():
                        metadata = self._load_backup_metadata(item)
                    else:
                        continue

                    backups.append({
                        "name": metadata["backup_name"],
                        "timestamp": metadata["timestamp"],
                        "description": metadata.get("description", ""),
                        "size": metadata.get("total_size", 0),
                        "components": list(metadata["components"].keys()),
                        "created_at": metadata["created_at"]
                    })

                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–å¤‡ä»½å…ƒæ•°æ®å¤±è´¥ {item.name}: {e}")

        # æŒ‰æ—¶é—´æ’åº
        backups.sort(key=lambda x: x["timestamp"], reverse=True)
        return backups

    async def schedule_backup(self):
        """è°ƒåº¦å®šæœŸå¤‡ä»½"""
        while True:
            try:
                await asyncio.sleep(self.backup_interval.total_seconds())

                if settings.BACKUP_ENABLED:
                    logger.info("ğŸ”„ å¼€å§‹å®šæœŸå¤‡ä»½")
                    await self.create_full_backup("å®šæœŸè‡ªåŠ¨å¤‡ä»½")
                else:
                    logger.debug("â¸ï¸ å¤‡ä»½åŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡å®šæœŸå¤‡ä»½")

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ å®šæœŸå¤‡ä»½å¤±è´¥: {e}")
                await asyncio.sleep(300)  # å¤±è´¥åç­‰å¾…5åˆ†é’Ÿå†é‡è¯•


# å…¨å±€å¤‡ä»½ç®¡ç†å™¨å®ä¾‹
backup_manager = BackupManager()