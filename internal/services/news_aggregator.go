package services

import (
	"crypto/md5"
	"fmt"
	"math"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"smart-stock-insider/internal/models"
)

// NewsAggregator æ–°é—»èšåˆå™¨
type NewsAggregator struct {
	rules      map[string]*models.NewsAggregationRule
	clusters   map[string]*models.NewsCluster
	duplicates map[string][]string // MD5å“ˆå¸Œ -> åŸå§‹æ–°é—»IDåˆ—è¡¨
	mutex      sync.RWMutex
	logger     *Logger
}

// NewNewsAggregator åˆ›å»ºæ–°é—»èšåˆå™¨
func NewNewsAggregator() *NewsAggregator {
	return &NewsAggregator{
		rules:      make(map[string]*models.NewsAggregationRule),
		clusters:   make(map[string]*models.NewsCluster),
		duplicates: make(map[string][]string),
		logger:     AppLogger,
	}
}

// LoadAggregationRules åŠ è½½èšåˆè§„åˆ™
func (na *NewsAggregator) LoadAggregationRules(rules map[string]*models.NewsAggregationRule) {
	na.mutex.Lock()
	defer na.mutex.Unlock()

	na.rules = rules
	na.logger.Info("åŠ è½½äº† %d æ¡æ–°é—»èšåˆè§„åˆ™", len(rules))
}

// ProcessNews å¤„ç†æ–°é—»èšåˆ
func (na *NewsAggregator) ProcessNews(news []*models.NewsItem) []*models.NewsItem {
	na.logger.Info("å¼€å§‹å¤„ç†æ–°é—»èšåˆï¼ŒåŸå§‹æ–°é—»æ•°é‡: %d", len(news))

	// 1. å»é‡å¤„ç†
	deduplicatedNews := na.deduplicateNews(news)

	// 2. ç›¸ä¼¼å†…å®¹èšç±»
	clusteredNews := na.clusterSimilarNews(deduplicatedNews)

	// 3. çƒ­é—¨è¯é¢˜è¯†åˆ«
	trendingNews := na.identifyTrendingTopics(clusteredNews)

	// 4. åº”ç”¨èšåˆè§„åˆ™
	finalNews := na.applyAggregationRules(trendingNews)

	na.logger.Info("æ–°é—»èšåˆå¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ–°é—»æ•°é‡: %d", len(finalNews))
	return finalNews
}

// deduplicateNews å»é‡æ–°é—»
func (na *NewsAggregator) deduplicateNews(news []*models.NewsItem) []*models.NewsItem {
	na.mutex.Lock()
	defer na.mutex.Unlock()

	seen := make(map[string]bool)
	var result []*models.NewsItem

	for _, item := range news {
		// ç”Ÿæˆå†…å®¹å“ˆå¸Œ
		hash := na.generateContentHash(item)

		// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼å†…å®¹
		if !na.isDuplicate(hash, item) {
			seen[hash] = true
			result = append(result, item)
			na.duplicates[hash] = append(na.duplicates[hash], item.ID)
		}
	}

	na.logger.Info("å»é‡å®Œæˆï¼Œå»é™¤é‡å¤æ–°é—» %d æ¡", len(news)-len(result))
	return result
}

// generateContentHash ç”Ÿæˆå†…å®¹å“ˆå¸Œ
func (na *NewsAggregator) generateContentHash(item *models.NewsItem) string {
	content := strings.ToLower(strings.TrimSpace(item.Title + item.Summary))

	// ç®€å•çš„å†…å®¹æ ‡å‡†åŒ–
	content = strings.ReplaceAll(content, " ", "")
	content = strings.ReplaceAll(content, "\n", "")
	content = strings.ReplaceAll(content, "\t", "")

	hash := md5.Sum([]byte(content))
	return fmt.Sprintf("%x", hash)
}

// isDuplicate æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹
func (na *NewsAggregator) isDuplicate(hash string, item *models.NewsItem) bool {
	// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå“ˆå¸Œ
	if _, exists := na.duplicates[hash]; exists {
		return true
	}

	// æ£€æŸ¥å»é‡è§„åˆ™
	for _, rule := range na.rules {
		if rule.RuleType == "duplicate" && rule.Enabled {
			if na.matchesDuplicateRule(rule, item) {
				return true
			}
		}
	}

	return false
}

// matchesDuplicateRule æ£€æŸ¥æ˜¯å¦åŒ¹é…å»é‡è§„åˆ™
func (na *NewsAggregator) matchesDuplicateRule(rule *models.NewsAggregationRule, item *models.NewsItem) bool {
	threshold := 0.9 // é»˜è®¤ç›¸ä¼¼åº¦é˜ˆå€¼

	// ä»è§„åˆ™æ¡ä»¶ä¸­è·å–é˜ˆå€¼
	if val, exists := rule.Conditions["similarity_threshold"]; exists {
		if parsed, err := strconv.ParseFloat(val, 64); err == nil {
			threshold = parsed
		}
	}

	// æ£€æŸ¥ä¸å·²æœ‰æ–°é—»çš„ç›¸ä¼¼åº¦
	for hash, ids := range na.duplicates {
		if len(ids) > 0 {
			// è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å®ç°å†…å®¹ç›¸ä¼¼åº¦è®¡ç®—
			if hash == na.generateContentHash(item) {
				return threshold >= 0.9 // å®Œå…¨ç›¸åŒ
			}
		}
	}

	return false
}

// clusterSimilarNews èšç±»ç›¸ä¼¼æ–°é—»
func (na *NewsAggregator) clusterSimilarNews(news []*models.NewsItem) []*models.NewsItem {
	na.mutex.Lock()
	defer na.mutex.Unlock()

	// æ¸…ç©ºä¹‹å‰çš„èšç±»
	na.clusters = make(map[string]*models.NewsCluster)

	// æŒ‰æ—¶é—´çª—å£åˆ†ç»„
	timeWindows := na.groupByTimeWindow(news, 2*time.Hour) // 2å°æ—¶çª—å£

	// åœ¨æ¯ä¸ªæ—¶é—´çª—å£å†…è¿›è¡Œèšç±»
	var clusteredNews []*models.NewsItem
	for windowKey, windowNews := range timeWindows {
		clusters := na.clusterBySimilarity(windowNews, 0.7)

		for _, cluster := range clusters {
			clusterID := fmt.Sprintf("cluster_%s_%d", windowKey, len(na.clusters))

			// åˆ›å»ºèšç±»å¯¹è±¡
			clusterObj := &models.NewsCluster{
				ID:         clusterID,
				Title:      na.generateClusterTitle(cluster),
				Summary:    na.generateClusterSummary(cluster),
				Category:   na.getClusterCategory(cluster),
				NewsIDs:    na.extractNewsIDs(cluster),
				StockCodes: na.extractStockCodes(cluster),
				Centroid:   cluster[0].ID, // ç¬¬ä¸€ä¸ªä½œä¸ºä¸­å¿ƒ
				Similarity:  na.calculateClusterSimilarity(cluster),
				ImpactScore: na.calculateClusterImpact(cluster),
				CreatedAt:  time.Now(),
				UpdatedAt:  time.Now(),
			}

			na.clusters[clusterID] = clusterObj

			// ä¿ç•™èšç±»ä¸­æœ€æœ‰ä»£è¡¨æ€§çš„æ–°é—»
			representativeNews := na.selectRepresentativeNews(cluster)
			clusteredNews = append(clusteredNews, representativeNews...)
		}
	}

	na.logger.Info("æ–°é—»èšç±»å®Œæˆï¼Œç”Ÿæˆ %d ä¸ªèšç±»", len(na.clusters))
	return clusteredNews
}

// groupByTimeWindow æŒ‰æ—¶é—´çª—å£åˆ†ç»„
func (na *NewsAggregator) groupByTimeWindow(news []*models.NewsItem, window time.Duration) map[string][]*models.NewsItem {
	groups := make(map[string][]*models.NewsItem)

	for _, item := range news {
		// æŒ‰å°æ—¶åˆ†ç»„
		windowKey := item.PublishTime.Truncate(window).Format("2006-01-02_15")
		groups[windowKey] = append(groups[windowKey], item)
	}

	return groups
}

// clusterBySimilarity æŒ‰ç›¸ä¼¼åº¦èšç±»
func (na *NewsAggregator) clusterBySimilarity(news []*models.NewsItem, threshold float64) [][]*models.NewsItem {
	var clusters [][]*models.NewsItem
	used := make(map[int]bool)

	for i, item := range news {
		if used[i] {
			continue
		}

		cluster := []*models.NewsItem{item}
		used[i] = true

		// æŸ¥æ‰¾ç›¸ä¼¼æ–°é—»
		for j := i + 1; j < len(news); j++ {
			if used[j] {
				continue
			}

			if na.calculateSimilarity(item, news[j]) >= threshold {
				cluster = append(cluster, news[j])
				used[j] = true
			}
		}

		clusters = append(clusters, cluster)
	}

	return clusters
}

// calculateSimilarity è®¡ç®—æ–°é—»ç›¸ä¼¼åº¦
func (na *NewsAggregator) calculateSimilarity(item1, item2 *models.NewsItem) float64 {
	title1 := strings.ToLower(item1.Title)
	title2 := strings.ToLower(item2.Title)

	// ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦è®¡ç®—
	intersection := 0
	words1 := strings.Fields(title1)
	words2 := strings.Fields(title2)

	wordSet1 := make(map[string]bool)
	wordSet2 := make(map[string]bool)

	for _, word := range words1 {
		wordSet1[word] = true
	}

	for _, word := range words2 {
		wordSet2[word] = true
	}

	// è®¡ç®—äº¤é›†
	for word := range wordSet1 {
		if wordSet2[word] {
			intersection++
		}
	}

	// è®¡ç®—å¹¶é›†å¤§å°
	union := len(wordSet1) + len(wordSet2) - intersection

	if union == 0 {
		return 0
	}

	// Jaccardç›¸ä¼¼åº¦
	return float64(intersection) / float64(union)
}

// generateClusterTitle ç”Ÿæˆèšç±»æ ‡é¢˜
func (na *NewsAggregator) generateClusterTitle(cluster []*models.NewsItem) string {
	if len(cluster) == 0 {
		return ""
	}

	if len(cluster) == 1 {
		return cluster[0].Title
	}

	// é€‰æ‹©æœ€é•¿æ ‡é¢˜ä½œä¸ºèšç±»æ ‡é¢˜
	var longestTitle string
	for _, item := range cluster {
		if len(item.Title) > len(longestTitle) {
			longestTitle = item.Title
		}
	}

	return fmt.Sprintf("ã€%dæ¡ç›¸å…³ã€‘%s", len(cluster), longestTitle)
}

// generateClusterSummary ç”Ÿæˆèšç±»æ‘˜è¦
func (na *NewsAggregator) generateClusterSummary(cluster []*models.NewsItem) string {
	if len(cluster) == 0 {
		return ""
	}

	if len(cluster) == 1 {
		return cluster[0].Summary
	}

	// åˆå¹¶æ‘˜è¦
	var combinedSummary strings.Builder
	combinedSummary.WriteString(fmt.Sprintf("å…±%dæ¡ç›¸å…³æ–°é—»ï¼š", len(cluster)))

	for i, item := range cluster {
		if i >= 3 { // æœ€å¤šæ˜¾ç¤º3æ¡æ‘˜è¦
			break
		}
		combinedSummary.WriteString(fmt.Sprintf("\n%d. %s", i+1, item.Title))
	}

	return combinedSummary.String()
}

// getClusterTitle è·å–èšç±»ç±»åˆ«
func (na *NewsAggregator) getClusterCategory(cluster []*models.NewsItem) string {
	if len(cluster) == 0 {
		return "general"
	}

	// ç»Ÿè®¡ç±»åˆ«å‡ºç°é¢‘ç‡
	categoryCount := make(map[string]int)
	for _, item := range cluster {
		categoryCount[item.Category]++
	}

	// è¿”å›æœ€é¢‘ç¹çš„ç±»åˆ«
	var maxCategory string
	var maxCount int
	for category, count := range categoryCount {
		if count > maxCount {
			maxCount = count
			maxCategory = category
		}
	}

	return maxCategory
}

// extractNewsIDs æå–æ–°é—»IDåˆ—è¡¨
func (na *NewsAggregator) extractNewsIDs(cluster []*models.NewsItem) []string {
	var ids []string
	for _, item := range cluster {
		ids = append(ids, item.ID)
	}
	return ids
}

// extractStockCodes æå–è‚¡ç¥¨ä»£ç 
func (na *NewsAggregator) extractStockCodes(cluster []*models.NewsItem) []string {
	codeSet := make(map[string]bool)
	var codes []string

	for _, item := range cluster {
		for _, code := range item.StockCodes {
			if !codeSet[code] {
				codeSet[code] = true
				codes = append(codes, code)
			}
		}
	}

	return codes
}

// calculateClusterSimilarity è®¡ç®—èšç±»ç›¸ä¼¼åº¦
func (na *NewsAggregator) calculateClusterSimilarity(cluster []*models.NewsItem) float64 {
	if len(cluster) < 2 {
		return 1.0
	}

	totalSimilarity := 0.0
	comparisons := 0

	for i := 0; i < len(cluster); i++ {
		for j := i + 1; j < len(cluster); j++ {
			similarity := na.calculateSimilarity(cluster[i], cluster[j])
			totalSimilarity += similarity
			comparisons++
		}
	}

	if comparisons == 0 {
		return 0
	}

	return totalSimilarity / float64(comparisons)
}

// calculateClusterImpact è®¡ç®—èšç±»å½±å“åŠ›
func (na *NewsAggregator) calculateClusterImpact(cluster []*models.NewsItem) float64 {
	if len(cluster) == 0 {
		return 0
	}

	// åŸºäºæ–°é—»æ•°é‡ã€ç›¸å…³æ€§å’Œæ—¶é—´æ–°é²œåº¦è®¡ç®—å½±å“åŠ›
	var totalRelevance float64
	var totalFreshness float64

	now := time.Now()
	for _, item := range cluster {
		totalRelevance += item.Relevance

		// æ—¶é—´æ–°é²œåº¦ï¼ˆè¶Šæ–°è¶Šæœ‰å½±å“åŠ›ï¼‰
		hoursAgo := now.Sub(item.PublishTime).Hours()
		freshness := math.Max(0, 1.0-hoursAgo/24.0) // 24å°æ—¶å†…æ–°é—»æ–°é²œåº¦é€’å‡
		totalFreshness += freshness
	}

	avgRelevance := totalRelevance / float64(len(cluster))
	avgFreshness := totalFreshness / float64(len(cluster))

	// ç»¼åˆå½±å“åŠ›è¯„åˆ†
	impact := math.Sqrt(float64(len(cluster))) * avgRelevance * avgFreshness

	// å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
	return math.Min(1.0, impact/10.0)
}

// selectRepresentativeNews é€‰æ‹©ä»£è¡¨æ€§æ–°é—»
func (na *NewsAggregator) selectRepresentativeNews(cluster []*models.NewsItem) []*models.NewsItem {
	if len(cluster) == 0 {
		return nil
	}

	if len(cluster) == 1 {
		return cluster
	}

	// æŒ‰å½±å“åŠ›å’Œç›¸å…³æ€§æ’åº
	sort.Slice(cluster, func(i, j int) bool {
		scoreI := cluster[i].Relevance * na.calculateFreshness(cluster[i])
		scoreJ := cluster[j].Relevance * na.calculateFreshness(cluster[j])
		return scoreI > scoreJ
	})

	// è¿”å›æœ€ä»£è¡¨æ€§çš„1-2æ¡æ–°é—»
	representativeCount := 1
	if len(cluster) >= 5 {
		representativeCount = 2
	}

	return cluster[:representativeCount]
}

// calculateFreshness è®¡ç®—æ–°é—»æ–°é²œåº¦
func (na *NewsAggregator) calculateFreshness(item *models.NewsItem) float64 {
	hoursAgo := time.Since(item.PublishTime).Hours()
	return math.Max(0, 1.0-hoursAgo/24.0)
}

// identifyTrendingTopics è¯†åˆ«çƒ­é—¨è¯é¢˜
func (na *NewsAggregator) identifyTrendingTopics(news []*models.NewsItem) []*models.NewsItem {
	na.mutex.Lock()
	defer na.mutex.Unlock()

	// æ£€æŸ¥çƒ­é—¨è¯é¢˜è§„åˆ™
	var trendingRule *models.NewsAggregationRule
	for _, rule := range na.rules {
		if rule.RuleType == "trending" && rule.Enabled {
			trendingRule = rule
			break
		}
	}

	if trendingRule == nil {
		return news
	}

	// æŒ‰æ—¶é—´çª—å£åˆ†ç»„æ–°é—»
	timeWindows := na.groupByTimeWindow(news, 2*time.Hour)

	// è¯†åˆ«çƒ­é—¨è¯é¢˜
	var trendingNews []*models.NewsItem
	for windowKey, windowNews := range timeWindows {
		topicClusters := na.identifyTopicsInWindow(windowNews, trendingRule)

		// ä¸ºæ¯ä¸ªçƒ­é—¨è¯é¢˜åˆ›å»ºèšåˆæ–°é—»
		for _, topic := range topicClusters {
			if len(topic) >= 3 { // è‡³å°‘3æ¡ç›¸å…³æ–°é—»æ‰ç®—çƒ­é—¨
				trendingItem := na.createTrendingNewsItem(topic, windowKey)
				trendingNews = append(trendingNews, trendingItem)
			}
		}
	}

	na.logger.Info("è¯†åˆ«åˆ° %d ä¸ªçƒ­é—¨è¯é¢˜", len(trendingNews))
	return append(news, trendingNews...)
}

// identifyTopicsInWindow åœ¨æ—¶é—´çª—å£å†…è¯†åˆ«è¯é¢˜
func (na *NewsAggregator) identifyTopicsInWindow(news []*models.NewsItem, rule *models.NewsAggregationRule) [][]*models.NewsItem {
	// åŸºäºå…³é”®è¯èšç±»
	keywordClusters := make(map[string][]*models.NewsItem)

	for _, item := range news {
		keywords := na.extractKeywords(item)
		for _, keyword := range keywords {
			keywordClusters[keyword] = append(keywordClusters[keyword], item)
		}
	}

	// ç­›é€‰å‡ºè¾¾åˆ°æœ€å°æ–‡ç« æ•°çš„èšç±»
	minArticles := 3
	if val, exists := rule.Conditions["min_articles"]; exists {
		if parsed, err := strconv.Atoi(val); err == nil {
			minArticles = parsed
		}
	}

	var topics [][]*models.NewsItem
	for _, cluster := range keywordClusters {
		if len(cluster) >= minArticles {
			topics = append(topics, cluster)
		}
	}

	return topics
}

// extractKeywords æå–å…³é”®è¯
func (na *NewsAggregator) extractKeywords(item *models.NewsItem) []string {
	var keywords []string

	// ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
	titleWords := strings.Fields(strings.ToLower(item.Title))
	for _, word := range titleWords {
		if len(word) > 2 && !na.isStopWord(word) {
			keywords = append(keywords, word)
		}
	}

	// ä»æ ‡ç­¾ä¸­æå–å…³é”®è¯
	for _, tag := range item.Tags {
		if len(tag) > 1 {
			keywords = append(keywords, strings.ToLower(tag))
		}
	}

	// ä»è‚¡ç¥¨ä»£ç ä¸­æå–
	for _, code := range item.StockCodes {
		keywords = append(keywords, strings.ToLower(code))
	}

	return keywords
}

// isStopWord åˆ¤æ–­æ˜¯å¦ä¸ºåœç”¨è¯
func (na *NewsAggregator) isStopWord(word string) bool {
	stopWords := map[string]bool{
		"çš„": true, "äº†": true, "åœ¨": true, "æ˜¯": true, "æˆ‘": true,
		"æœ‰": true, "å’Œ": true, "å°±": true, "ä¸": true, "äºº": true,
		"éƒ½": true, "ä¸€": true, "ä¸ª": true, "ä¸Š": true, "ä¹Ÿ": true,
		"å¾ˆ": true, "åˆ°": true, "è¯´": true, "è¦": true, "å»": true,
		"ä½ ": true, "ä¼š": true, "ç€": true, "æ²¡æœ‰": true, "çœ‹": true,
		"å¥½": true, "è‡ªå·±": true, "è¿™": true, "é‚£": true, "å°±æ˜¯": true,
		"the": true, "a": true, "an": true, "and": true, "or": true,
		"but": true, "in": true, "on": true, "at": true, "to": true,
		"for": true, "of": true, "with": true, "by": true, "as": true,
	}

	return stopWords[strings.TrimSpace(word)]
}

// createTrendingNewsItem åˆ›å»ºçƒ­é—¨è¯é¢˜æ–°é—»é¡¹
func (na *NewsAggregator) createTrendingNewsItem(cluster []*models.NewsItem, windowKey string) *models.NewsItem {
	// èšåˆä¿¡æ¯
	title := fmt.Sprintf("ğŸ”¥ çƒ­é—¨è¯é¢˜ï¼š%s", na.generateClusterTitle(cluster))
	summary := fmt.Sprintf("åœ¨%sæœŸé—´ï¼Œå…±å‘ç°%dæ¡ç›¸å…³æ–°é—»ï¼š", windowKey, len(cluster))

	// æå–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
	var allCodes []string
	codeSet := make(map[string]bool)
	for _, item := range cluster {
		for _, code := range item.StockCodes {
			if !codeSet[code] {
				codeSet[code] = true
				allCodes = append(allCodes, code)
			}
		}
	}

	// åˆ›å»ºçƒ­é—¨è¯é¢˜æ–°é—»é¡¹
	trendingItem := &models.NewsItem{
		ID:        fmt.Sprintf("trending_%s_%d", windowKey, len(cluster)),
		Title:     title,
		Summary:   summary,
		Content:   na.generateTrendingContent(cluster),
		Source:    "æ™ºè‚¡é€šèšåˆ",
		Author:    "ç³»ç»Ÿè‡ªåŠ¨èšåˆ",
		URL:       "",
		PublishTime: time.Now(),
		Category:  "çƒ­é—¨è¯é¢˜",
		Tags:      []string{"çƒ­é—¨è¯é¢˜", "èšåˆ", "è¶‹åŠ¿"},
		Relevance: 1.0,
		StockCodes: allCodes,
		Sentiment: &models.SentimentResult{
			Label:      "trending",
			Score:      0.0,
			Confidence: 0.8,
			Emotions:   make(map[string]float64),
		},
		CreatedAt: time.Now(),
		UpdatedAt: time.Now(),
	}

	return trendingItem
}

// generateTrendingContent ç”Ÿæˆçƒ­é—¨è¯é¢˜å†…å®¹
func (na *NewsAggregator) generateTrendingContent(cluster []*models.NewsItem) string {
	var content strings.Builder
	content.WriteString("çƒ­é—¨è¯é¢˜è¯¦æƒ…ï¼š\n\n")

	for i, item := range cluster {
		if i >= 5 { // æœ€å¤šæ˜¾ç¤º5æ¡
			break
		}
		content.WriteString(fmt.Sprintf("%d. %s\n", i+1, item.Title))
		content.WriteString(fmt.Sprintf("   æ¥æºï¼š%s\n", item.Source))
		content.WriteString(fmt.Sprintf("   æ—¶é—´ï¼š%s\n", item.PublishTime.Format("15:04")))
		if item.Summary != "" {
			content.WriteString(fmt.Sprintf("   æ‘˜è¦ï¼š%s\n", item.Summary))
		}
		content.WriteString("\n")
	}

	if len(cluster) > 5 {
		content.WriteString(fmt.Sprintf("...è¿˜æœ‰%dæ¡ç›¸å…³æ–°é—»\n", len(cluster)-5))
	}

	return content.String()
}

// applyAggregationRules åº”ç”¨èšåˆè§„åˆ™
func (na *NewsAggregator) applyAggregationRules(news []*models.NewsItem) []*models.NewsItem {
	// è¿™é‡Œå¯ä»¥å®ç°æ›´å¤šçš„èšåˆè§„åˆ™å¤„ç†
	// ç›®å‰ä¸»è¦ä¾èµ–å‰é¢çš„å»é‡ã€èšç±»å’Œçƒ­é—¨è¯é¢˜è¯†åˆ«

	// æŒ‰æ—¶é—´å€’åºæ’åˆ—
	sort.Slice(news, func(i, j int) bool {
		return news[i].PublishTime.After(news[j].PublishTime)
	})

	return news
}

// GetClusters è·å–èšç±»ä¿¡æ¯
func (na *NewsAggregator) GetClusters() map[string]*models.NewsCluster {
	na.mutex.RLock()
	defer na.mutex.RUnlock()

	result := make(map[string]*models.NewsCluster)
	for k, v := range na.clusters {
		result[k] = v
	}
	return result
}

// GetDuplicates è·å–é‡å¤æ–°é—»ä¿¡æ¯
func (na *NewsAggregator) GetDuplicates() map[string][]string {
	na.mutex.RLock()
	defer na.mutex.RUnlock()

	result := make(map[string][]string)
	for k, v := range na.duplicates {
		result[k] = append([]string{}, v...)
	}
	return result
}

// è¾…åŠ©å‡½æ•°
func parseFloat(s string) float64 {
	var f float64
	fmt.Sscanf(s, "%f", &f)
	return f
}

func parseInt(s string) int {
	var i int
	fmt.Sscanf(s, "%d", &i)
	return i
}